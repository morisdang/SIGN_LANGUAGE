{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rYukTjf7WzJc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "66XMldQ2TcLB",
        "outputId": "2c636ab3-b290-4e04-8c80-23e85f14fb9f"
      },
      "outputs": [],
      "source": [
        "# Target Arrays Processed Input Videos\n",
        "X = np.zeros([N_SAMPLES, N_TARGET_FRAMES, N_COLS], dtype=np.float32)\n",
        "# Ordinally Encoded Target With value 59 for pad token\n",
        "y = np.full(shape=[N_SAMPLES, N_TARGET_FRAMES], fill_value=N_UNIQUE_CHARACTERS, dtype=np.int8)\n",
        "# Phrase Type\n",
        "y_phrase_type = np.empty(shape=[N_SAMPLES], dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "wVRJKcFZVbb4",
        "outputId": "27158b4d-e806-4991-fd12-0adf08f37384"
      },
      "outputs": [],
      "source": [
        "# All Unique Parquet Files\n",
        "UNIQUE_FILE_PATHS = pd.Series(train['file_path'].unique())\n",
        "N_UNIQUE_FILE_PATHS = len(UNIQUE_FILE_PATHS)\n",
        "# Counter to keep track of sample\n",
        "row = 0\n",
        "count = 0\n",
        "# Compressed Parquet Files\n",
        "Path('train_landmark_subsets').mkdir(parents=True, exist_ok=True)\n",
        "# Number Of Frames Per Character\n",
        "N_FRAMES_PER_CHARACTER = []\n",
        "# Minimum Number Of Frames Per Character\n",
        "MIN_NUM_FRAMES_PER_CHARACTER = 4\n",
        "VALID_IDXS = []\n",
        "\n",
        "# Fill Arrays\n",
        "for idx, file_path in enumerate(tqdm(UNIQUE_FILE_PATHS)):\n",
        "    # Progress Logging\n",
        "    print(f'Processed {idx:02d}/{N_UNIQUE_FILE_PATHS} parquet files')\n",
        "    # Read parquet file\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Save COLUMN Subset of parquet files for TFLite Model verficiation\n",
        "    name = file_path.split('/')[-1]\n",
        "    if idx < 10:\n",
        "        df[COLUMNS0].to_parquet(f'train_landmark_subsets/{name}', engine='pyarrow', compression='zstd')\n",
        "    # Iterate Over Samples\n",
        "    for group, group_df in df.groupby('sequence_id'):\n",
        "        # Number of Frames Per Character\n",
        "        n_frames_per_character =  len(group_df[COLUMNS0].values) / len(train_sequence_id.loc[group, 'phrase_char'])\n",
        "        N_FRAMES_PER_CHARACTER.append(n_frames_per_character)\n",
        "        if n_frames_per_character < MIN_NUM_FRAMES_PER_CHARACTER:\n",
        "            count = count + 1\n",
        "            continue\n",
        "        else:\n",
        "            # Add Valid Index\n",
        "            VALID_IDXS.append(count)\n",
        "            count = count + 1\n",
        "\n",
        "        # Get Processed Frames and non empty frame indices\n",
        "        frames = preprocess_layer(group_df[COLUMNS0].values)\n",
        "        assert frames.ndim == 2\n",
        "        # Assign\n",
        "        X[row] = frames\n",
        "        # Add Target By Ordinally Encoding Characters\n",
        "        phrase_char = train_sequence_id.loc[group, 'phrase_char']\n",
        "        for col, char in enumerate(phrase_char):\n",
        "            y[row, col] = CHAR2ORD.get(char)\n",
        "        # Add EOS Token\n",
        "        y[row, col+1] = EOS_TOKEN\n",
        "        # Phrase Type\n",
        "        y_phrase_type[row] = train_sequence_id.loc[group, 'phrase_type']\n",
        "        # Row Count\n",
        "        row += 1\n",
        "    # clean up\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "xzXY_0jtVxhh",
        "outputId": "4e638023-db07-4223-826d-7c51559a46e5"
      },
      "outputs": [],
      "source": [
        "# Save X/y\n",
        "np.save('X.npy', X)\n",
        "np.save('y.npy', y)\n",
        "# Save Validation\n",
        "splitter = GroupShuffleSplit(test_size=0.10, n_splits=2, random_state=SEED)\n",
        "PARTICIPANT_IDS = train['participant_id'].values[VALID_IDXS]\n",
        "train_idxs, val_idxs = next(splitter.split(X, y, groups=PARTICIPANT_IDS))\n",
        "\n",
        "# Save Train\n",
        "np.save('X_train.npy', X[train_idxs])\n",
        "np.save('y_train.npy', y[train_idxs])\n",
        "# Save Validation\n",
        "np.save('X_val.npy', X[val_idxs])\n",
        "np.save('y_val.npy', y[val_idxs])\n",
        "# Verify Train/Val is correctly split by participan id\n",
        "print(f'Patient ID Intersection Train/Val: {set(PARTICIPANT_IDS[train_idxs]).intersection(PARTICIPANT_IDS[val_idxs])}')\n",
        "# Train/Val Sizes\n",
        "print(f'# Train Samples: {len(train_idxs)}, # Val Samples: {len(val_idxs)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "tpdkGQITV3ic",
        "outputId": "9ac85c8a-41b0-41b1-916c-f74959a8dc4e"
      },
      "outputs": [],
      "source": [
        "def get_left_right_hand_mean_std():\n",
        "    # Dominant Hand Statistics\n",
        "    MEANS = np.zeros([N_COLS], dtype=np.float32)\n",
        "    STDS = np.zeros([N_COLS], dtype=np.float32)\n",
        "\n",
        "    # Iterate over all landmarks\n",
        "    for col, v in enumerate(tqdm(X.reshape([-1, N_COLS]).T)):\n",
        "        v = v[np.nonzero(v)]\n",
        "        # Remove zero values as they are NaN values\n",
        "        MEANS[col] = v.astype(np.float32).mean()\n",
        "        STDS[col] = v.astype(np.float32).std()\n",
        "        if col in LEFT_HAND_IDXS:\n",
        "            axes[0].boxplot(v, notch=False, showfliers=False, positions=[col], whis=[5,95])\n",
        "        elif col in RIGHT_HAND_IDXS:\n",
        "            axes[1].boxplot(v, notch=False, showfliers=False, positions=[col], whis=[5,95])\n",
        "        else:\n",
        "            axes[2].boxplot(v, notch=False, showfliers=False, positions=[col], whis=[5,95])\n",
        "\n",
        "    return MEANS, STDS\n",
        "\n",
        "# Get Dominant Hand Mean/Standard Deviation\n",
        "MEANS, STDS = get_left_right_hand_mean_std()\n",
        "# Save Mean/STD to normalize input in neural network model\n",
        "np.save('MEANS.npy', MEANS)\n",
        "np.save('STDS.npy', STDS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
